"""
ëª¨ë¸ íŒŒì¼ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
ì¶”ì¶œëœ íŒŒì¼ë“¤ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.
"""

import os
import json
import pickle
import torch
import numpy as np
from pathlib import Path
from sklearn.preprocessing import StandardScaler, LabelEncoder


def verify_model_files(models_dir=None):
    """
    ëª¨ë¸ íŒŒì¼ë“¤ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    
    Args:
        models_dir: ëª¨ë¸ íŒŒì¼ì´ ìžˆëŠ” ë””ë ‰í† ë¦¬ (Noneì´ë©´ ìžë™ íƒìƒ‰)
    """
    print("="*80)
    print("ëª¨ë¸ íŒŒì¼ ê²€ì¦ ì‹œìž‘")
    print("="*80)
    
    # ë””ë ‰í† ë¦¬ ì°¾ê¸°
    if models_dir is None:
        # ê°€ëŠ¥í•œ ê²½ë¡œë“¤
        possible_dirs = [
            Path(__file__).parent.parent / "backend" / "models",
            Path(os.getcwd()) / "rag-chatbot" / "backend" / "models",
            Path(os.getcwd()) / "backend" / "models",
            Path(os.getcwd()) / "models",
            Path("rag-chatbot/backend/models"),
            Path("backend/models"),
            Path("models"),
        ]
        
        # í˜„ìž¬ ë””ë ‰í† ë¦¬ì—ì„œ ìž¬ê·€ì ìœ¼ë¡œ ì°¾ê¸°
        current_dir = Path(os.getcwd())
        for pattern in ["**/model_config.json", "**/scaler.pkl"]:
            for found_file in current_dir.glob(pattern):
                found_dir = found_file.parent
                if found_dir not in possible_dirs:
                    possible_dirs.append(found_dir)
        
        for dir_path in possible_dirs:
            if dir_path.exists() and (dir_path / "model_config.json").exists():
                models_dir = dir_path
                break
        
        if models_dir is None:
            print("âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("\në‹¤ìŒ ìœ„ì¹˜ë“¤ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤:")
            for dir_path in possible_dirs:
                exists = dir_path.exists()
                has_config = (dir_path / "model_config.json").exists() if exists else False
                status = "âœ“" if has_config else ("ì¡´ìž¬" if exists else "ì—†ìŒ")
                print(f"  {status} {dir_path.absolute()}")
            
            print("\nðŸ’¡ í•´ê²° ë°©ë²•:")
            print("  1. íŒŒì¼ì´ ìžˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ì§ì ‘ ì§€ì •:")
            print("     python verify_model_files.py /path/to/models")
            print("  2. ë˜ëŠ” íŒŒì¼ë“¤ì„ ë‹¤ìŒ ìœ„ì¹˜ì— ë³µì‚¬:")
            print(f"     {Path(__file__).parent.parent / 'backend' / 'models'}")
            return False
    else:
        models_dir = Path(models_dir)
    
    print(f"\nâœ“ ëª¨ë¸ ë””ë ‰í† ë¦¬: {models_dir.absolute()}")
    
    # í•„ìˆ˜ íŒŒì¼ ëª©ë¡
    required_files = {
        "model_config.json": "ëª¨ë¸ ì„¤ì •",
        "scaler.pkl": "StandardScaler",
        "temporal_encoder.pkl": "Temporal Encoder",
        "spatial_encoder.pkl": "Spatial Encoder",
        "TimeSeriesTransformer_best.pth": "ëª¨ë¸ ê°€ì¤‘ì¹˜"
    }
    
    print("\n" + "="*80)
    print("íŒŒì¼ ì¡´ìž¬ í™•ì¸")
    print("="*80)
    
    missing_files = []
    existing_files = {}
    
    for filename, description in required_files.items():
        filepath = models_dir / filename
        if filepath.exists():
            size = filepath.stat().st_size
            existing_files[filename] = filepath
            print(f"âœ“ {filename:40s} ({size:>10,} bytes) - {description}")
        else:
            missing_files.append(filename)
            print(f"âœ— {filename:40s} {'ì—†ìŒ':>10} - {description}")
    
    if missing_files:
        print(f"\nâŒ ëˆ„ë½ëœ íŒŒì¼ {len(missing_files)}ê°œ: {missing_files}")
        return False
    
    print("\nâœ“ ëª¨ë“  í•„ìˆ˜ íŒŒì¼ì´ ì¡´ìž¬í•©ë‹ˆë‹¤!")
    
    # íŒŒì¼ ë‚´ìš© ê²€ì¦
    print("\n" + "="*80)
    print("íŒŒì¼ ë‚´ìš© ê²€ì¦")
    print("="*80)
    
    try:
        # 1. Config íŒŒì¼ ê²€ì¦
        print("\n[1] model_config.json ê²€ì¦ ì¤‘...")
        with open(existing_files["model_config.json"], 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # í•„ìˆ˜ í‚¤ í™•ì¸
        required_keys = [
            "model_hyperparameters",
            "features",
            "preprocessing",
            "encoders"
        ]
        
        for key in required_keys:
            if key not in config:
                print(f"  âœ— í•„ìˆ˜ í‚¤ ì—†ìŒ: {key}")
                return False
        
        print("  âœ“ JSON êµ¬ì¡° ì˜¬ë°”ë¦„")
        print(f"  âœ“ d_model: {config['model_hyperparameters']['d_model']}")
        print(f"  âœ“ seq_len: {config['model_hyperparameters']['seq_len']}")
        print(f"  âœ“ cyano_vars ê°œìˆ˜: {len(config['features']['cyano_vars'])}")
        print(f"  âœ“ wq_vars ê°œìˆ˜: {len(config['features']['wq_vars'])}")
        print(f"  âœ“ feature_order ê°œìˆ˜: {len(config['features']['feature_order'])}")
        print(f"  âœ“ log1p_applied: {config['preprocessing']['log1p_applied']}")
        
        # 2. Scaler ê²€ì¦
        print("\n[2] scaler.pkl ê²€ì¦ ì¤‘...")
        with open(existing_files["scaler.pkl"], 'rb') as f:
            scaler = pickle.load(f)
        
        if not isinstance(scaler, StandardScaler):
            print(f"  âœ— íƒ€ìž… ì˜¤ë¥˜: StandardScalerê°€ ì•„ë‹˜ (ì‹¤ì œ: {type(scaler)})")
            return False
        
        print("  âœ“ StandardScaler íƒ€ìž… ì˜¬ë°”ë¦„")
        if hasattr(scaler, 'mean_') and scaler.mean_ is not None:
            print(f"  âœ“ mean_ shape: {scaler.mean_.shape}")
        if hasattr(scaler, 'scale_') and scaler.scale_ is not None:
            print(f"  âœ“ scale_ shape: {scaler.scale_.shape}")
        
        # 3. Temporal Encoder ê²€ì¦
        print("\n[3] temporal_encoder.pkl ê²€ì¦ ì¤‘...")
        with open(existing_files["temporal_encoder.pkl"], 'rb') as f:
            temporal_encoder = pickle.load(f)
        
        if not isinstance(temporal_encoder, LabelEncoder):
            print(f"  âœ— íƒ€ìž… ì˜¤ë¥˜: LabelEncoderê°€ ì•„ë‹˜ (ì‹¤ì œ: {type(temporal_encoder)})")
            return False
        
        print("  âœ“ LabelEncoder íƒ€ìž… ì˜¬ë°”ë¦„")
        print(f"  âœ“ í´ëž˜ìŠ¤ ìˆ˜: {len(temporal_encoder.classes_)}")
        print(f"  âœ“ í´ëž˜ìŠ¤ ìƒ˜í”Œ: {temporal_encoder.classes_[:5].tolist()}...")
        
        # 4. Spatial Encoder ê²€ì¦
        print("\n[4] spatial_encoder.pkl ê²€ì¦ ì¤‘...")
        with open(existing_files["spatial_encoder.pkl"], 'rb') as f:
            spatial_encoder = pickle.load(f)
        
        if not isinstance(spatial_encoder, LabelEncoder):
            print(f"  âœ— íƒ€ìž… ì˜¤ë¥˜: LabelEncoderê°€ ì•„ë‹˜ (ì‹¤ì œ: {type(spatial_encoder)})")
            return False
        
        print("  âœ“ LabelEncoder íƒ€ìž… ì˜¬ë°”ë¦„")
        print(f"  âœ“ í´ëž˜ìŠ¤ ìˆ˜: {len(spatial_encoder.classes_)}")
        print(f"  âœ“ í´ëž˜ìŠ¤ ìƒ˜í”Œ: {spatial_encoder.classes_[:5].tolist()}...")
        
        # 5. ëª¨ë¸ ê°€ì¤‘ì¹˜ ê²€ì¦
        print("\n[5] TimeSeriesTransformer_best.pth ê²€ì¦ ì¤‘...")
        try:
            state_dict = torch.load(existing_files["TimeSeriesTransformer_best.pth"], 
                                  map_location='cpu', 
                                  weights_only=False)
            
            if not isinstance(state_dict, dict):
                print(f"  âœ— íƒ€ìž… ì˜¤ë¥˜: dictê°€ ì•„ë‹˜ (ì‹¤ì œ: {type(state_dict)})")
                return False
            
            print("  âœ“ PyTorch state_dict íƒ€ìž… ì˜¬ë°”ë¦„")
            print(f"  âœ“ ê°€ì¤‘ì¹˜ í‚¤ ê°œìˆ˜: {len(state_dict)}")
            
            # ì£¼ìš” ë ˆì´ì–´ í™•ì¸
            expected_keys = [
                "input_projection.weight",
                "temporal_embedding.weight",
                "spatial_embedding.weight",
                "transformer.layers.0.self_attn.in_proj_weight",
                "output_head.1.weight"
            ]
            
            found_keys = []
            for key in expected_keys:
                if any(key in k for k in state_dict.keys()):
                    found_keys.append(key)
            
            print(f"  âœ“ ì˜ˆìƒ ë ˆì´ì–´ {len(found_keys)}/{len(expected_keys)}ê°œ ë°œê²¬")
            
            # ê°€ì¤‘ì¹˜ shape í™•ì¸
            sample_key = list(state_dict.keys())[0]
            print(f"  âœ“ ìƒ˜í”Œ í‚¤: {sample_key}, shape: {state_dict[sample_key].shape}")
            
        except Exception as e:
            print(f"  âœ— ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
        
        # 6. Configì™€ Encoder ì¼ì¹˜ í™•ì¸
        print("\n[6] Configì™€ Encoder ì¼ì¹˜ í™•ì¸ ì¤‘...")
        config_temporal_count = config['encoders']['num_temporal_categories']
        config_spatial_count = config['encoders']['num_spatial_categories']
        actual_temporal_count = len(temporal_encoder.classes_)
        actual_spatial_count = len(spatial_encoder.classes_)
        
        if config_temporal_count != actual_temporal_count:
            print(f"  âœ— Temporal ì¹´í…Œê³ ë¦¬ ìˆ˜ ë¶ˆì¼ì¹˜: config={config_temporal_count}, encoder={actual_temporal_count}")
            return False
        
        if config_spatial_count != actual_spatial_count:
            print(f"  âœ— Spatial ì¹´í…Œê³ ë¦¬ ìˆ˜ ë¶ˆì¼ì¹˜: config={config_spatial_count}, encoder={actual_spatial_count}")
            return False
        
        print(f"  âœ“ Temporal ì¹´í…Œê³ ë¦¬ ìˆ˜ ì¼ì¹˜: {actual_temporal_count}")
        print(f"  âœ“ Spatial ì¹´í…Œê³ ë¦¬ ìˆ˜ ì¼ì¹˜: {actual_spatial_count}")
        
        # 7. Feature ìˆœì„œ í™•ì¸
        print("\n[7] Feature ìˆœì„œ í™•ì¸ ì¤‘...")
        feature_order = config['features']['feature_order']
        cyano_vars = config['features']['cyano_vars']
        wq_vars = config['features']['wq_vars']
        
        expected_order = cyano_vars + wq_vars
        
        if feature_order != expected_order:
            print(f"  âš  Feature ìˆœì„œê°€ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤.")
            print(f"    Config: {feature_order[:3]}...")
            print(f"    ì˜ˆìƒ: {expected_order[:3]}...")
        else:
            print(f"  âœ“ Feature ìˆœì„œ ì˜¬ë°”ë¦„: {len(feature_order)}ê°œ")
            print(f"    - cyano_vars: {len(cyano_vars)}ê°œ")
            print(f"    - wq_vars: {len(wq_vars)}ê°œ")
        
        print("\n" + "="*80)
        print("âœ… ëª¨ë“  íŒŒì¼ ê²€ì¦ ì™„ë£Œ!")
        print("="*80)
        print("\nìš”ì•½:")
        print(f"  - ëª¨ë¸ ë””ë ‰í† ë¦¬: {models_dir.absolute()}")
        print(f"  - íŒŒì¼ ê°œìˆ˜: {len(existing_files)}ê°œ")
        print(f"  - ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°: d_model={config['model_hyperparameters']['d_model']}, seq_len={config['model_hyperparameters']['seq_len']}")
        print(f"  - Feature ê°œìˆ˜: {len(feature_order)}ê°œ")
        print(f"  - Temporal ì¹´í…Œê³ ë¦¬: {actual_temporal_count}ê°œ")
        print(f"  - Spatial ì¹´í…Œê³ ë¦¬: {actual_spatial_count}ê°œ")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    import sys
    
    # ëª…ë ¹ì¤„ ì¸ìžë¡œ ë””ë ‰í† ë¦¬ ì§€ì • ê°€ëŠ¥
    if len(sys.argv) > 1:
        models_dir = Path(sys.argv[1])
    else:
        models_dir = None
    
    success = verify_model_files(models_dir)
    sys.exit(0 if success else 1)

